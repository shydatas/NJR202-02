# version: "3.0"  # 使用 Docker Compose 的版本 3.0，適合大部分部署場景

services:

    worker:  # 定義一個服務，名稱為 crawler_twse
        image: shydatas/data_ingestion:latest  # 使用的映像檔名稱與標籤（版本）
        container_name: "scraper_worker"  # 設定 container_name = crawler_hahow_worker  
        command: uv run celery -A data_ingestion.message_queue.worker worker --loglevel=info --hostname=worker%h
        # 啟動容器後執行的命令，這裡是啟動 Celery worker，指定 app 為 data_ingestion.worker，設定日誌等級為 info，

        restart: always  # 若容器停止或崩潰，自動重新啟動
        environment:
            - TZ=Asia/Taipei  # 設定時區為台北（UTC+8）
            - RABBITMQ_HOST=rabbitmq
        networks:
            - njr20202_network  # 將此服務連接到 my_network 網路
        volumes:
            - ./output:/app/output

networks:
    njr20202_network:
        external: true
